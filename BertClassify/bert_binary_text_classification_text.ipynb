{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "bert_binary_text_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHtj4yMXiwjP",
        "colab_type": "text"
      },
      "source": [
        "# BERT Binary Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gipK9XhXi0JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install transformers -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYZBQ9DmiwjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv, os, sys, logging, time, pickle, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers.optimization import AdamW\n",
        "from transformers.optimization import get_linear_schedule_with_warmup as WarmupLinearSchedule\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw89ki3gwNUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://120.79.8.250:8080/dataset/train.csv  #-O datasets/yelp_review_polarity/train.csv\n",
        "!wget http://120.79.8.250:8080/dataset/test.csv #-O datasets/yelp_review_polarity/test.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_YQe-FWiwjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
        "DATA_DIR = \"/content/\" # \"datasets/yelp_review_polarity/\"\n",
        "# This is where BERT will look for pre-trained models to load parameters from.\n",
        "CACHE_DIR = \"/content/cache/\"\n",
        "# The name of the task to train. I'm going to name this 'yelp'.\n",
        "TASK_NAME = '/content/yelp'\n",
        "# The output directory where the fine-tuned model and checkpoints will be written.\n",
        "OUTPUT_DIR = f'/content/outputs/{TASK_NAME}/'\n",
        "# The directory where the evaluation reports will be written to.\n",
        "REPORTS_DIR = f'/content/reports/{TASK_NAME}_evaluation_reports/'\n",
        "WEIGHTS_NAME = \"/content/pytorch_model.bin\"\n",
        "CONFIG_NAME = \"/content/config.json\"\n",
        "\n",
        "# The following variables are for training.\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "GRADIENT_ACCUMULATION_STEPS = 1 \n",
        "NUM_TRAIN_EPOCHS = 1\n",
        "# The maximum total input sequence length after WordPiece tokenization.\n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "OUTPUT_MODE = 'classification'\n",
        "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
        "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
        "# bert-base-multilingual-cased, bert-base-chinese.\n",
        "BERT_MODEL = \"bert-base-cased\"\n",
        "LEARNING_RATE = 2e-5\n",
        "# Deprecated for `BertAdam`\n",
        "# WARMUP_PROPORTION = 0.1 \n",
        "NUM_WARMUP_STEPS = 100\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "# More variables for evaluation.\n",
        "EVAL_BATCH_SIZE = 8\n",
        "RANDOM_SEED = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShIW-GCviwjX",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation\n",
        "We are going to use [the Yelp Review Polarity dataset](https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz) to train our binary text classification model. First load the dataset in with `pandas` and take a look at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N7AwDOCiwjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(DATA_DIR + \"train.csv\", header=None)\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARV42--Wiwjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(DATA_DIR + \"test.csv\", header=None)\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mpchv4wiwjj",
        "colab_type": "text"
      },
      "source": [
        "BERT, however, wants data to be in a `tsv` file with a specific format as given below (Four columns, and no header row).\n",
        "\n",
        "* *Column 0:* An ID for the row\n",
        "* *Column 1:* The label for the row (should be an int)\n",
        "* *Column 2:* A column of the same letter for all rows. BERT wants this so we’ll give it, but we don’t have a use for it.\n",
        "* *Column 3:* The text for the row\n",
        "\n",
        "Let's make things a little BERT-friendly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBP4ZnSWiwjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_bert = pd.DataFrame({\n",
        "    \"id\": range(len(train_df)),\n",
        "    \"label\": train_df[3],\n",
        "    \"alpha\": [\"a\"] * train_df.shape[0],\n",
        "    \"text\": train_df[2].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "train_df_bert.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9OMKWPmiwjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_df_bert = pd.DataFrame({\n",
        "    \"id\": range(len(test_df)),\n",
        "    \"label\": test_df[3],\n",
        "    \"alpha\": [\"a\"] * test_df.shape[0],\n",
        "    \"text\": test_df[2].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "dev_df_bert.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZWFJh-miwjo",
        "colab_type": "text"
      },
      "source": [
        "For convenience, I've named the test data as dev data. The convenience stems from the fact that BERT comes with data loading classes that expects **train** and **dev** files in the above format. We can use the train data to train our model, and the dev data to evaluate its performance. \n",
        "\n",
        "BERT's data loading classes can also use a **test** file but it expects the test file to be unlabelled.\n",
        "\n",
        "Now that we have the data in the correct form, all we need to do is to save the train and dev data as `.tsv` files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVHa7XC0iwjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_bert.to_csv(DATA_DIR + 'train.tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD9zyJx6iwjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_df_bert.to_csv(DATA_DIR + 'dev.tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kExBkFOSiwj-",
        "colab_type": "text"
      },
      "source": [
        "## Data to Features\n",
        "The final step before fine-tuning is to convert the data into features that BERT uses.\n",
        "\n",
        "The first class, `InputExample`, is the format that a single example of our dataset should be in. We won't be using the `text_b` attribute since that is not necessary for our binary classification task.\n",
        "\n",
        "The other two classes, `DataProcessor` and `BinaryClassificationProcessor`, are helper classes that can be used to read in `.tsv` files and prepare them to be converted into features that will ultimately be fed into the actual BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQMNvj5biwkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Increase CSV reader's field limit in case we have long text\n",
        "csv.field_size_limit(2147483647)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"\n",
        "        Construct an `InputExample`.\n",
        "        \n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "        \n",
        "class DataProcessor(object):\n",
        "    \"\"\"\n",
        "    Base class for data converters for sequence classification data sets.\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "class BinaryClassificationProcessor(DataProcessor):\n",
        "    \"\"\"\n",
        "    Processor for binary classification dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            label = line[1]\n",
        "            examples.append(InputExample(guid=guid, \n",
        "                                         text_a=text_a, \n",
        "                                         text_b=None, \n",
        "                                         label=label))\n",
        "        return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7n_PxvZiwkN",
        "colab_type": "text"
      },
      "source": [
        "So far, we have the capability to read in `.tsv` datasets and convert them into `InputExample` objects. BERT, being a neural network, cannot directly deal with text as we have in `InputExample` objects. The next step is to convert them into `InputFeatures`.\n",
        "\n",
        "BERT has a constraint on the maximum length of a sequence after tokenizing. For any BERT model, the maximum sequence length after tokenization is $512$. But we can set any sequence length equal to or below this value. For faster training, I'll be using $128$ as the maximum sequence length. A bigger number may give better results if there are sequences longer than this value.\n",
        "\n",
        "An `InputFeatures` consists of purely numerical data (with the proper sequence lengths) that can then be fed into the BERT model. This is prepared by tokenizing the text of each example and truncating the longer sequence while padding the shorter sequences to the given maximum sequence length ($128$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKNhw23NiwkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"\n",
        "    Truncates a sequence pair in place to the maximum length.\n",
        "    \"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence one token at a time. \n",
        "    # This makes more sense than truncating an equal percent of tokens from each, since if one sequence \n",
        "    # is very short then each token that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "def convert_example_to_feature(example_row):\n",
        "    # Input `example_row`\n",
        "    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "        # Modify `tokens_a` and `tokens_b` in place so that the total length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "    segment_ids = [0] * len(tokens)\n",
        "\n",
        "    if tokens_b:\n",
        "        tokens += tokens_b + [\"[SEP]\"]\n",
        "        segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length\n",
        "    padding = [0] * (max_seq_length - len(input_ids))\n",
        "    input_ids += padding\n",
        "    input_mask += padding\n",
        "    segment_ids += padding\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    if output_mode == \"classification\":\n",
        "        label_id = label_map[example.label]\n",
        "    elif output_mode == \"regression\":\n",
        "        label_id = float(example.label)\n",
        "    else:\n",
        "        raise KeyError(output_mode)\n",
        "\n",
        "    return InputFeatures(input_ids=input_ids,\n",
        "                         input_mask=input_mask,\n",
        "                         segment_ids=segment_ids,\n",
        "                         label_id=label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dwJoxkniwkV",
        "colab_type": "text"
      },
      "source": [
        "## Pickling\n",
        "In the following cells, we are going to use our `BinaryClassificationProcessor` to load in the data, and get everything ready for the tokenization step. Our goal is to create a list of tuples, `train_examples_for_processing`, and then run `convert_example_to_feature` for each item of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZg-3_ibiwkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = BinaryClassificationProcessor()\n",
        "train_examples = processor.get_train_examples(DATA_DIR)\n",
        "# print(train_examples[:8])\n",
        "# [<__main__.InputExample object at 0x000001568D222FD0>, <__main__.InputExample object at 0x00000156BF287D68>, <__main__.InputExample object at 0x00000156BF579630>, <__main__.InputExample object at 0x00000156BF5796A0>, <__main__.InputExample object at 0x00000156BF579710>, <__main__.InputExample object at 0x00000156BF579780>, <__main__.InputExample object at 0x00000156BF5797F0>, <__main__.InputExample object at 0x00000156BF579860>]\n",
        "\n",
        "train_examples_len = len(train_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vqbVvR4iwkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [0, 1] for binary classification\n",
        "label_list = processor.get_labels() \n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JetlE4j-iwkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_optimization_steps = int(train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfunQ2Mziwkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVeoENOuiwkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = {label: i for i, label in enumerate(label_list)}\n",
        "train_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]\n",
        "\n",
        "# train_examples_for_processing = train_examples_for_processing[:8]\n",
        "# print(train_examples_for_processing)\n",
        "# [(<__main__.InputExample object at 0x000001568D1E0C50>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x000001568D1E0DA0>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x00000156CC75EAC8>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x00000156CC75EB38>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x00000156CC75EBA8>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x00000156CC75EC18>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x00000156CC75EC88>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification'), (<__main__.InputExample object at 0x00000156CC75ECF8>, {'0': 0, '1': 1}, 128, <transformers.tokenization_bert.BertTokenizer object at 0x00000156FF5976D8>, 'classification')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okTFe1eriwkt",
        "colab_type": "text"
      },
      "source": [
        "We set some variables that we'll use while training the model. Next, we loaded the pretrained tokenizer by BERT. In this case, we'll be using the `bert-base-cased` model.\n",
        "\n",
        "\n",
        "The `convert_example_to_feature` function expects a tuple containing an example, the label map, the maximum sequence length, a tokenizer, and the output mode. So lastly, we will create an examples list ready to be processed (tokenized, truncated/padded, and turned into `InputFeatures`) by the `convert_example_to_feature` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNa_vfJ-iwkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "print(f'Converting {train_examples_len} examples: \\n')\n",
        "\n",
        "train_features = []\n",
        "num_examples_processed = 0\n",
        "\n",
        "for example in train_examples_for_processing:\n",
        "    train_features.append(convert_example_to_feature(example))\n",
        "    num_examples_processed += 1\n",
        "    if (num_examples_processed % 10000 == 0):\n",
        "        print(f\"{num_examples_processed} examples have been processed.\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Finish in %s seconds.\" % (time.time() - start_time))\n",
        "# print(train_features)\n",
        "# [<__main__.InputFeatures object at 0x00000156C7ED0E48>, <__main__.InputFeatures object at 0x00000156BF2FD668>, <__main__.InputFeatures object at 0x00000156BF2D2860>, <__main__.InputFeatures object at 0x00000156C267F588>, <__main__.InputFeatures object at 0x00000156C7ED0A58>, <__main__.InputFeatures object at 0x00000156BF29E978>, <__main__.InputFeatures object at 0x00000156BF2D2D30>, <__main__.InputFeatures object at 0x00000156AAAD3A20>]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMg4DOOgiwky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(DATA_DIR + \"train_features.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_features, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33rlo_3piwk1",
        "colab_type": "text"
      },
      "source": [
        "Once all the examples are converted into features, we can pickle them to disk for safekeeping. Next time, you can just unpickle the file to get the list of features.\n",
        "\n",
        "## Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owB2LuCFiwk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkAtP12uiwk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKMlb1Xxiwk7",
        "colab_type": "text"
      },
      "source": [
        "HuggingFace's PyTorch implementation of BERT comes with a function that automatically downloads the BERT model for us. The model will be downloaded into a cache folder.\n",
        "\n",
        "We just need to do a tiny bit more configuration for the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P67DsmrMiwk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "print(optimizer_grouped_parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "f8xUqHm0iwk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `BertAdam` has been deprecated\n",
        "# optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "#                      lr=LEARNING_RATE,\n",
        "#                      warmup=WARMUP_PROPORTION,\n",
        "#                      t_total=num_train_optimization_steps)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, \n",
        "                  lr=LEARNING_RATE,\n",
        "                  correct_bias=False)\n",
        "scheduler = WarmupLinearSchedule(optimizer, num_warmup_steps= NUM_WARMUP_STEPS,num_training_steps =num_train_optimization_steps)\n",
        "\n",
        "print(optimizer)\n",
        "print()\n",
        "print(scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SJZpC_ciwlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "nb_tr_steps = 0\n",
        "tr_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtVwV4xtiwlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_features = pickle.load(open(\"datasets/yelp_review_polarity/train_features.pkl\", \"rb\"))\n",
        "with open(DATA_DIR + \"train_features.pkl\", \"rb\") as f:\n",
        "    train_features = pickle.load(f)\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\", train_examples_len)\n",
        "print(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "print(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "print(\"\\nInput ids:\")\n",
        "print(all_input_ids)\n",
        "\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "print(\"\\nInput mask:\")\n",
        "print(all_input_mask)\n",
        "\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "print(\"\\nSegment ids:\")\n",
        "print(all_segment_ids)\n",
        "\n",
        "if OUTPUT_MODE == \"classification\":\n",
        "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "elif OUTPUT_MODE == \"regression\":\n",
        "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)\n",
        "print(\"\\nLabel ids:\")\n",
        "print(all_label_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU8T95dQiwlF",
        "colab_type": "text"
      },
      "source": [
        "Setup our `DataLoader` for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_JIuVutiwlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfEFdl-TiwlI",
        "colab_type": "text"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMSAYYKtiwlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "for _ in tqdm(range(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids = batch\n",
        "        \n",
        "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
        "        # print(logits)\n",
        "        # (tensor([[-0.2749,  0.3726],\n",
        "        #         [-0.0907,  0.1621],\n",
        "        #         [-0.0432,  0.1179],\n",
        "        #         [ 0.3068,  0.2338],\n",
        "        #         [-0.0388,  0.1583],\n",
        "        #         [ 0.8389,  0.0382],\n",
        "        #         [-0.2859,  0.1343],\n",
        "        #         [ 0.5700, -0.0209],\n",
        "        #         [ 0.0744,  0.1734],\n",
        "        #         [-0.0615,  0.1675],\n",
        "        #         [ 0.2898,  0.2939],\n",
        "        #         [ 0.2406,  0.1009]], device='cuda:0', grad_fn=<AddmmBackward>),)\n",
        "        \n",
        "        # print(logits[0])\n",
        "        # tensor([[ 0.1675,  0.0322],\n",
        "        #        [-0.3478,  0.2567],\n",
        "        #        [ 0.1478, -0.0364],\n",
        "        #        [-0.1464,  0.2940],\n",
        "        #        [-0.0275,  0.2472],\n",
        "        #        [ 0.1564,  0.2518],\n",
        "        #        [ 0.1667,  0.0801],\n",
        "        #        [ 0.7791,  0.0669],\n",
        "        #        [-0.2284,  0.1228],\n",
        "        #        [-0.1749,  0.1484],\n",
        "        #        [-0.0431,  0.1213],\n",
        "        #        [-0.0634, -0.0488]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
        "        \n",
        "        if OUTPUT_MODE == \"classification\":\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits[0].view(-1, num_labels), label_ids.view(-1))\n",
        "        elif OUTPUT_MODE == \"regression\":\n",
        "            loss_fct = MSELoss()\n",
        "            loss = loss_fct(logits[0].view(-1), label_ids.view(-1))\n",
        "\n",
        "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "        loss.backward()\n",
        "        print(\"\\r%f\" % loss, end='')\n",
        "        \n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "            clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)  \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z72qtO5iwlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only save the model itself\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n",
        "\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(OUTPUT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9koHom4iwmB",
        "colab_type": "text"
      },
      "source": [
        "Now we've trained the BERT model for one epoch, we can evaluate the results. Of course, more training will likely yield better results but even one epoch should be sufficient for proof of concept.\n",
        "\n",
        "Save the model, configuration file, and vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_72bau7iwmC",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "Most of the code for the evaluation is very similar to the training process, so I won’t go into too much detail but I’ll list some important points.\n",
        "\n",
        "* `BERT_MODEL` parameter should be the name of your fine-tuned model.\n",
        "* The tokenizer should be loaded from the vocabulary file created in the training stage. In our case, that would `outputs/yelp/vocab.txt`.\n",
        "* This time, we'll be using the `BinaryClassificationProcessor` to load in the `dev.tsv` file by calling the `get_dev_examples` method.\n",
        "* Double check to make sure you are loading the fine-tuned model and not the original BERT model. Frankly speaking, I did it and got surprised with a really bad result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNqJVPhiwmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the report directory; in this case, `yelp_evaluation_reports`\n",
        "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n",
        "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
        "    os.makedirs(REPORTS_DIR)\n",
        "\n",
        "if not os.path.exists(REPORTS_DIR):\n",
        "    os.makedirs(REPORTS_DIR)\n",
        "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
        "    os.makedirs(REPORTS_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH_bpbB-iwmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_eval_report(task_name, labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    return {\n",
        "        \"task\": task_name,\n",
        "        \"mcc\": mcc,\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn\n",
        "    }\n",
        "\n",
        "def compute_metrics(task_name, labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "    return get_eval_report(task_name, labels, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9j9UVAZiwmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(OUTPUT_DIR + 'vocab.txt', do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wff1y0SsiwmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = BinaryClassificationProcessor()\n",
        "eval_examples = processor.get_dev_examples(DATA_DIR)\n",
        "# [0, 1] for binary classification\n",
        "label_list = processor.get_labels() \n",
        "num_labels = len(label_list)\n",
        "eval_examples_len = len(eval_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgWe7dVWiwmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = {label: i for i, label in enumerate(label_list)}\n",
        "eval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ei0ZCpriwmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "print(f'Converting {eval_examples_len} examples. \\n')\n",
        "\n",
        "eval_features = []\n",
        "\n",
        "for example in eval_examples_for_processing[:10000]:\n",
        "    eval_features.append(convert_example_to_feature(example))\n",
        "\n",
        "print(\"Finish in %s seconds.\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaCgs5GiiwmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "\n",
        "if OUTPUT_MODE == \"classification\":\n",
        "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
        "elif OUTPUT_MODE == \"regression\":\n",
        "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb7zhRyOiwmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "# Run prediction for full data\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlDW_ZoliwmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertForSequenceClassification.from_pretrained(OUTPUT_DIR, num_labels=len(label_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se09_M13iwmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRZ6POzKiwma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "eval_loss = 0\n",
        "nb_eval_steps = 0\n",
        "preds = []\n",
        "\n",
        "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    input_ids = input_ids.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    segment_ids = segment_ids.to(device)\n",
        "    label_ids = label_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
        "\n",
        "    # create eval loss and other metric required by the task\n",
        "    if OUTPUT_MODE == \"classification\":\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        tmp_eval_loss = loss_fct(logits[0].view(-1, num_labels), label_ids.view(-1))\n",
        "    elif OUTPUT_MODE == \"regression\":\n",
        "        loss_fct = MSELoss()\n",
        "        tmp_eval_loss = loss_fct(logits[0].view(-1), label_ids.view(-1))\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    nb_eval_steps += 1\n",
        "    if len(preds) == 0:\n",
        "        preds.append(logits[0].detach().cpu().numpy())\n",
        "    else:\n",
        "        preds[0] = np.append(preds[0], logits[0].detach().cpu().numpy(), axis=0)\n",
        "        \n",
        "eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "# print(\"preds:\", preds)\n",
        "# preds: [array([[-0.07383163,  0.07358287],\n",
        "#        [ 0.26556033, -0.2577362 ],\n",
        "#        [-0.04184964, -0.05931795],\n",
        "#        ...,\n",
        "#        [ 0.2624964 , -0.11659782],\n",
        "#        [ 0.6032646 , -0.3702591 ],\n",
        "#        [ 0.6068794 , -0.34930488]], dtype=float32)]\n",
        "preds = preds[0]\n",
        "# print(\"preds:\", preds)\n",
        "# preds: [[-0.07383163  0.07358287]\n",
        "#        [ 0.26556033 -0.2577362 ]\n",
        "#        [-0.04184964 -0.05931795]\n",
        "#        ...\n",
        "#        [ 0.2624964  -0.11659782]\n",
        "#        [ 0.6032646  -0.3702591 ]\n",
        "#        [ 0.6068794  -0.34930488]]\n",
        "\n",
        "if OUTPUT_MODE == \"classification\":\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "elif OUTPUT_MODE == \"regression\":\n",
        "    preds = np.squeeze(preds)\n",
        "    \n",
        "# print(preds)\n",
        "# [1 0 0 ... 0 0 0]\n",
        "\n",
        "# print(all_label_ids)\n",
        "# tensor([1, 0, 1,  ..., 0, 1, 0])\n",
        "\n",
        "# print(all_label_ids.numpy())\n",
        "# [1 0 1 ... 0 1 0]\n",
        "\n",
        "result = compute_metrics(TASK_NAME, all_label_ids.numpy(), preds)\n",
        "\n",
        "result['eval_loss'] = eval_loss\n",
        "\n",
        "output_eval_file = os.path.join(REPORTS_DIR, \"eval_results.txt\")\n",
        "with open(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in (result.keys()):\n",
        "        print(\"  %s = %s\" % (key, str(result[key])))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-5cdW9hiwmd",
        "colab_type": "text"
      },
      "source": [
        "With just one single epoch of training, our BERT model achieves a $0.913$ Matthews correlation coefficient (good measure for evaluating unbalanced datasets according to `sklearn` doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)). With more training, and perhaps some hyperparameter tuning, we can almost certainly improve upon what is already an impressive score.\n",
        "\n",
        "BERT is an incredibly powerful language representation model that shows great promise in a wide variety of NLP tasks."
      ]
    }
  ]
}