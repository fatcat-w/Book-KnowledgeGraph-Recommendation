{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from data_loader import RSDataset, KGDataset\n",
    "from train import train\n",
    "\n",
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bdbbe8b6370e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#                     help='path to store training summary')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# args = parser.parse_args()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mrs_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRSDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mkg_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKGDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# train(args, rs_dataset, kg_dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# # book\n",
    "# parser.add_argument('--dataset', type=str, default='book', help='which dataset to use')\n",
    "# parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n",
    "# parser.add_argument('--dim', type=int, default=8, help='dimension of user and entity embeddings')\n",
    "# parser.add_argument('--L', type=int, default=1, help='number of low layers')\n",
    "# parser.add_argument('--H', type=int, default=1, help='number of high layers')\n",
    "# parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "# parser.add_argument('--l2_weight', type=float, default=1e-6, help='weight of l2 regularization')\n",
    "# parser.add_argument('--lr_rs', type=float, default=2e-4, help='learning rate of RS task')\n",
    "# parser.add_argument('--lr_kge', type=float, default=2e-5, help='learning rate of KGE task')\n",
    "# parser.add_argument('--kge_interval', type=int, default=2, help='training interval of KGE task')\n",
    "\n",
    "# parser.add_argument('--workers', type=int, default=3,\n",
    "#                     help='number of data loading workers')\n",
    "# parser.add_argument('-sl', '--show_loss', action='store_true',\n",
    "#                     help='show loss or not')\n",
    "# parser.add_argument('-st', '--show_topk', action='store_true',\n",
    "#                     help='show topK or not')\n",
    "# parser.add_argument('-sum', '--summary_path', type=str, default='./summary',\n",
    "#                     help='path to store training summary')\n",
    "# args = parser.parse_args()\n",
    "rs_dataset = RSDataset(args)\n",
    "kg_dataset = KGDataset(args)\n",
    "# train(args, rs_dataset, kg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import MKR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def train(args, rs_dataset, kg_dataset):\n",
    "\n",
    "    show_loss = args.show_loss\n",
    "    show_topk = args.show_topk\n",
    "\n",
    "    # Get RS data\n",
    "    n_user = rs_dataset.n_user\n",
    "    n_item = rs_dataset.n_item\n",
    "    train_data, eval_data, test_data = rs_dataset.data\n",
    "    train_indices, eval_indices, test_indices = rs_dataset.indices\n",
    "\n",
    "    # Get KG data\n",
    "    n_entity = kg_dataset.n_entity\n",
    "    n_relation = kg_dataset.n_relation\n",
    "    kg = kg_dataset.kg\n",
    "\n",
    "    # Init train sampler\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "\n",
    "    # Init MKR model\n",
    "    model = MKR(args, n_user, n_item, n_entity, n_relation)\n",
    "\n",
    "    # Init Sumwriter\n",
    "    writer = SummaryWriter(args.summary_path)\n",
    "\n",
    "    # Top-K evaluation settings\n",
    "    user_num = 100\n",
    "    k_list = [1, 2, 5, 10, 20, 50, 100]\n",
    "    train_record = get_user_record(train_data, True)\n",
    "    test_record = get_user_record(test_data, False)\n",
    "    user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
    "    if len(user_list) > user_num:\n",
    "        user_list = np.random.choice(user_list, size=user_num, replace=False)\n",
    "    item_set = set(list(range(n_item)))\n",
    "    step = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "        print(\"Train RS\")\n",
    "        train_loader = DataLoader(rs_dataset, batch_size=args.batch_size,\n",
    "                                  num_workers=args.workers, sampler=train_sampler)\n",
    "        for i, rs_batch_data in enumerate(train_loader):\n",
    "            loss, base_loss_rs, l2_loss_rs = model.train_rs(rs_batch_data)\n",
    "            writer.add_scalar(\"rs_loss\", loss.cpu().detach().numpy(), global_step=step)\n",
    "            writer.add_scalar(\"rs_base_loss\", base_loss_rs.cpu().detach().numpy(), global_step=step)\n",
    "            writer.add_scalar(\"rs_l2_loss\", l2_loss_rs.cpu().detach().numpy(), global_step=step)\n",
    "            step += 1\n",
    "            if show_loss:\n",
    "                print(loss)\n",
    "\n",
    "        if epoch % args.kge_interval == 0:\n",
    "            print(\"Train KGE\")\n",
    "            kg_train_loader = DataLoader(kg_dataset, batch_size=args.batch_size,\n",
    "                                         num_workers=args.workers, shuffle=True)\n",
    "            for i, kg_batch_data in enumerate(kg_train_loader):\n",
    "                rmse, loss_kge, base_loss_kge, l2_loss_kge = model.train_kge(kg_batch_data)\n",
    "                writer.add_scalar(\"kge_rmse_loss\", rmse.cpu().detach().numpy(), global_step=step)\n",
    "                writer.add_scalar(\"kge_loss\", loss_kge.cpu().detach().numpy(), global_step=step)\n",
    "                writer.add_scalar(\"kge_base_loss\", base_loss_kge.cpu().detach().numpy(), global_step=step)\n",
    "                writer.add_scalar(\"kge_l2_loss\", l2_loss_kge.cpu().detach().numpy(), global_step=step)\n",
    "                step += 1\n",
    "                if show_loss:\n",
    "                    print(rmse)\n",
    "\n",
    "\n",
    "        # CTR evaluation\n",
    "        train_auc, train_acc = model.eval(train_data)\n",
    "        eval_auc, eval_acc = model.eval(eval_data)\n",
    "        test_auc, test_acc = model.eval(test_data)\n",
    "\n",
    "        print('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "              % (epoch, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "\n",
    "        # top-K evaluation\n",
    "        if show_topk:\n",
    "            precision, recall, f1 = model.topk_eval(user_list, train_record, test_record, item_set, k_list)\n",
    "            print('precision: ', end='')\n",
    "            for i in precision:\n",
    "                print('%.4f\\t' % i, end='')\n",
    "            print()\n",
    "            print('recall: ', end='')\n",
    "            for i in recall:\n",
    "                print('%.4f\\t' % i, end='')\n",
    "            print()\n",
    "            print('f1: ', end='')\n",
    "            for i in f1:\n",
    "                print('%.4f\\t' % i, end='')\n",
    "            print('\\n')\n",
    "\n",
    "def get_user_record(data, is_train):\n",
    "    user_history_dict = dict()\n",
    "    for interaction in data:\n",
    "        user = interaction[0]\n",
    "        item = interaction[1]\n",
    "        label = interaction[2]\n",
    "        if is_train or label == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = set()\n",
    "            user_history_dict[user].add(item)\n",
    "    return user_history_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensor]",
   "language": "python",
   "name": "conda-env-tensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
